{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aishwarya-ps-kumbla/MST_Aish_practice_problems_01/blob/main/ICT_LPRNet_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fH8SnUS1Oov",
        "outputId": "68099cb3-69a7-4963-8a57-5b096193ca79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LPRNet_ICT'...\n",
            "remote: Enumerating objects: 66, done.\u001b[K\n",
            "remote: Counting objects: 100% (66/66), done.\u001b[K\n",
            "remote: Compressing objects: 100% (61/61), done.\u001b[K\n",
            "remote: Total 66 (delta 29), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (66/66), 207.87 KiB | 1.10 MiB/s, done.\n",
            "Resolving deltas: 100% (29/29), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/deepak-dgj270/LPRNet_ICT"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/LPRNet_ICT/train.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbyW-6S_2sAz",
        "outputId": "971b3263-0432-475b-afc7-daba97e7ea5d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-08-02 15:03:07.425037: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-08-02 15:03:07.443120: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-08-02 15:03:07.448578: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-08-02 15:03:07.462022: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-08-02 15:03:08.586432: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1722610990.065324    1231 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "I0000 00:00:1722610990.115586    1231 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "I0000 00:00:1722610990.115908    1231 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "I0000 00:00:1722610990.116645    1231 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "I0000 00:00:1722610990.116873    1231 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "I0000 00:00:1722610990.117061    1231 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "I0000 00:00:1722610990.206576    1231 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "I0000 00:00:1722610990.206859    1231 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-08-02 15:03:10.207014: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "I0000 00:00:1722610990.207133    1231 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-08-02 15:03:10.207294: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13949 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "['KA01ML6902_2.JPG', 'KA01MP2936_4.jpg', 'KA01ML5359_1.JPG', '.ipynb_checkpoints', 'KA01MP2936_3.JPG', 'KA01MR2693_5.JPG', 'KA53MD7540_6.JPG']\n",
            "KA01ML6902\n",
            "KA01MP2936\n",
            "KA01ML5359\n",
            "KA01MP2936\n",
            "KA01MR2693\n",
            "KA53MD7540\n",
            "['.ipynb_checkpoints']\n",
            "Train Len is 6\n",
            "Training ...\n",
            "Start of epoch 0 / 1000\n",
            "2024-08-02 15:03:11.837652: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8906\n",
            "W0000 00:00:1722610992.286014    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610992.372798    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610992.373991    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610992.375219    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610992.376426    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610992.413224    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610992.414441    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610992.415588    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610992.433809    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610992.435107    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610992.436360    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610992.437582    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610992.438858    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610992.456398    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610992.509135    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610993.482925    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610993.484164    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610993.485383    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610993.486593    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610993.487763    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610993.488928    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610993.490092    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610993.491369    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610993.492553    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610993.493899    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610993.495124    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610993.497102    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610993.498331    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610993.499622    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610993.564530    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610993.596572    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610993.597769    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610993.598955    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610993.600137    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610993.601308    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610993.602496    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610993.603703    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610993.604892    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610993.606134    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610993.607369    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610993.608605    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610993.609873    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610993.611207    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610993.612563    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610993.614324    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610993.644011    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610993.645425    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610993.646642    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610993.647863    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610993.649100    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610993.650337    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610993.651532    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610993.652744    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610993.654021    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610993.655221    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610993.656442    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610993.657736    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610993.659082    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610993.660478    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610993.661736    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610993.674719    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610993.675943    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610993.677175    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610993.678401    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610993.679619    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610993.680814    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610993.682010    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610993.683220    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610993.684500    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610993.685747    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610993.687010    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610993.688323    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610993.689694    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610993.691032    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610993.692297    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610994.744173    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610994.745397    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610994.746587    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610994.747804    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610994.748988    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610994.750178    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610994.751421    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610994.752658    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610994.753847    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610994.755072    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610994.756259    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610994.757469    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610994.758694    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610994.760086    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610994.762000    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610994.786334    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610994.789025    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610994.791282    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610994.793543    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610994.795341    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610994.797649    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610994.799648    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610994.801756    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610994.803855    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610994.827375    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610994.828914    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610994.830471    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610994.832183    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.283736    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.285348    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.286630    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.288137    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.289524    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.323974    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.325541    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.326938    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.333130    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.470712    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.479109    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.480544    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.507229    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.508667    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.519168    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.520535    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.522135    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.523669    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.525684    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.572139    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.670719    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.680964    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.682190    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.683570    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.684853    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.686446    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.688095    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.689403    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.690664    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.695340    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.698424    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.699695    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.700907    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.702156    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.703337    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.704708    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.706057    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.707455    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.708996    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.755637    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.756886    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.758180    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.759510    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.760798    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.762114    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.763407    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.764736    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.767297    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.771145    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.772470    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.773781    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.775214    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.776846    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.778609    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.780912    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.783356    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.786223    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.797395    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.798674    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.799997    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.802976    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.804309    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.805682    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.807038    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.808489    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.809872    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.814881    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.816402    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.817826    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.819486    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.820752    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.822380    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.823818    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.825992    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.828641    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.861617    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.862640    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.863738    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.864793    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.865825    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.866882    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.867940    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.869030    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.871954    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.875104    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.876198    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.877276    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.878726    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.880364    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.881537    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.882829    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.884981    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.887557    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.889028    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.902076    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.903123    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.904244    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.905248    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.906315    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.907370    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.908446    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.909503    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.912484    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.913728    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.917631    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.919904    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.921217    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.922335    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.923958    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.924999    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.927648    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.929091    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.951318    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.952322    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.953309    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.954291    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.955643    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.964181    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.965440    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.966903    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.968265    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.969575    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.973087    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.974736    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.978704    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.979711    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.980764    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.982155    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.983196    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.984806    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.986001    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.987117    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.988365    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.995946    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610995.998093    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1722610996.001098    1231 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "Train loss 279.614501953125, time 5.7513041496276855 \n",
            "\n",
            "Start of epoch 1 / 1000\n",
            "Train loss 275.8287353515625, time 0.5380568504333496 \n",
            "\n",
            "Start of epoch 2 / 1000\n",
            "Train loss 274.48614501953125, time 0.5413272380828857 \n",
            "\n",
            "Start of epoch 3 / 1000\n",
            "Train loss 271.28485107421875, time 0.5897760391235352 \n",
            "\n",
            "Start of epoch 4 / 1000\n",
            "Train loss 266.5189208984375, time 0.5527198314666748 \n",
            "\n",
            "Start of epoch 5 / 1000\n",
            "Train loss 263.5816650390625, time 0.5396749973297119 \n",
            "\n",
            "Start of epoch 6 / 1000\n",
            "Train loss 262.29840087890625, time 0.39055871963500977 \n",
            "\n",
            "Start of epoch 7 / 1000\n",
            "Train loss 252.89959716796875, time 0.4339711666107178 \n",
            "\n",
            "Start of epoch 8 / 1000\n",
            "Train loss 248.39239501953125, time 0.4018082618713379 \n",
            "\n",
            "Start of epoch 9 / 1000\n",
            "Train loss 243.65066528320312, time 0.40392327308654785 \n",
            "\n",
            "Start of epoch 10 / 1000\n",
            "Train loss 235.0482177734375, time 0.41554927825927734 \n",
            "\n",
            "Start of epoch 11 / 1000\n",
            "Train loss 230.64990234375, time 0.3948993682861328 \n",
            "\n",
            "Start of epoch 12 / 1000\n",
            "Train loss 224.54444885253906, time 0.4292609691619873 \n",
            "\n",
            "Start of epoch 13 / 1000\n",
            "Train loss 219.33865356445312, time 0.39537858963012695 \n",
            "\n",
            "Start of epoch 14 / 1000\n",
            "Train loss 214.95252990722656, time 0.3913426399230957 \n",
            "\n",
            "Start of epoch 15 / 1000\n",
            "Train loss 206.87777709960938, time 0.4167211055755615 \n",
            "\n",
            "Start of epoch 16 / 1000\n",
            "Train loss 202.08685302734375, time 0.3901810646057129 \n",
            "\n",
            "Start of epoch 17 / 1000\n",
            "Train loss 202.18316650390625, time 0.377918004989624 \n",
            "\n",
            "Start of epoch 18 / 1000\n",
            "Train loss 195.19912719726562, time 0.40044498443603516 \n",
            "\n",
            "Start of epoch 19 / 1000\n",
            "Train loss 188.7046661376953, time 0.3927578926086426 \n",
            "\n",
            "Start of epoch 20 / 1000\n",
            "Train loss 184.12652587890625, time 0.40877270698547363 \n",
            "\n",
            "Start of epoch 21 / 1000\n",
            "Train loss 178.05361938476562, time 0.3865077495574951 \n",
            "\n",
            "Start of epoch 22 / 1000\n",
            "Train loss 168.50950622558594, time 0.39583539962768555 \n",
            "\n",
            "Start of epoch 23 / 1000\n",
            "Train loss 166.39080810546875, time 0.4118492603302002 \n",
            "\n",
            "Start of epoch 24 / 1000\n",
            "Train loss 160.58734130859375, time 0.39687442779541016 \n",
            "\n",
            "Start of epoch 25 / 1000\n",
            "Train loss 159.61672973632812, time 0.4055664539337158 \n",
            "\n",
            "/content/LPRNet_ICT/evaluate.py:17: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  return(np.sum(values )/self.val_batch_len)\n",
            "Number of samples in test set: 0\n",
            "mean loss: nan\n",
            "mean CER: nan\n",
            "WER: nan\n",
            "\n",
            "Validation loss is greater than best_val_loss\n",
            "Start of epoch 26 / 1000\n",
            "Train loss 155.36483764648438, time 0.39108848571777344 \n",
            "\n",
            "Start of epoch 27 / 1000\n",
            "Train loss 150.7530517578125, time 0.3941822052001953 \n",
            "\n",
            "Start of epoch 28 / 1000\n",
            "Train loss 147.14453125, time 0.4081761837005615 \n",
            "\n",
            "Start of epoch 29 / 1000\n",
            "Train loss 142.85523986816406, time 0.3923008441925049 \n",
            "\n",
            "Start of epoch 30 / 1000\n",
            "Train loss 138.79556274414062, time 0.49791550636291504 \n",
            "\n",
            "Start of epoch 31 / 1000\n",
            "Train loss 138.04571533203125, time 0.5290853977203369 \n",
            "\n",
            "Start of epoch 32 / 1000\n",
            "Train loss 135.9482421875, time 0.5409328937530518 \n",
            "\n",
            "Start of epoch 33 / 1000\n",
            "Train loss 133.7130584716797, time 0.5891983509063721 \n",
            "\n",
            "Start of epoch 34 / 1000\n",
            "Train loss 129.0469207763672, time 0.5902516841888428 \n",
            "\n",
            "Start of epoch 35 / 1000\n",
            "Train loss 124.8882064819336, time 0.4380357265472412 \n",
            "\n",
            "Start of epoch 36 / 1000\n",
            "Train loss 123.12841796875, time 0.38905811309814453 \n",
            "\n",
            "Start of epoch 37 / 1000\n",
            "Train loss 122.50872039794922, time 0.39864230155944824 \n",
            "\n",
            "Start of epoch 38 / 1000\n",
            "Train loss 116.91475677490234, time 0.38900041580200195 \n",
            "\n",
            "Start of epoch 39 / 1000\n",
            "Train loss 115.40335083007812, time 0.4069178104400635 \n",
            "\n",
            "Start of epoch 40 / 1000\n",
            "Train loss 117.04413604736328, time 0.389812707901001 \n",
            "\n",
            "Start of epoch 41 / 1000\n",
            "Train loss 113.69157409667969, time 0.40411853790283203 \n",
            "\n",
            "Start of epoch 42 / 1000\n",
            "Train loss 112.34632110595703, time 0.4068031311035156 \n",
            "\n",
            "Start of epoch 43 / 1000\n",
            "Train loss 107.38493347167969, time 0.3940253257751465 \n",
            "\n",
            "Start of epoch 44 / 1000\n",
            "Train loss 110.90733337402344, time 0.4065120220184326 \n",
            "\n",
            "Start of epoch 45 / 1000\n",
            "Train loss 104.84988403320312, time 0.3928401470184326 \n",
            "\n",
            "Start of epoch 46 / 1000\n",
            "Train loss 102.52609252929688, time 0.3925340175628662 \n",
            "\n",
            "Start of epoch 47 / 1000\n",
            "Train loss 105.26847839355469, time 0.40164947509765625 \n",
            "\n",
            "Start of epoch 48 / 1000\n",
            "Train loss 100.64340209960938, time 0.39202189445495605 \n",
            "\n",
            "Start of epoch 49 / 1000\n",
            "Train loss 108.92147827148438, time 0.4097254276275635 \n",
            "\n",
            "Start of epoch 50 / 1000\n",
            "Train loss 100.11294555664062, time 0.3867168426513672 \n",
            "\n",
            "Number of samples in test set: 0\n",
            "mean loss: nan\n",
            "mean CER: nan\n",
            "WER: nan\n",
            "\n",
            "Validation loss is greater than best_val_loss\n",
            "Start of epoch 51 / 1000\n",
            "Train loss 95.87080383300781, time 0.4031558036804199 \n",
            "\n",
            "Start of epoch 52 / 1000\n",
            "Train loss 98.02572631835938, time 0.4088006019592285 \n",
            "\n",
            "Start of epoch 53 / 1000\n",
            "Train loss 93.59256744384766, time 0.4014449119567871 \n",
            "\n",
            "Start of epoch 54 / 1000\n",
            "Train loss 95.65473937988281, time 0.40494346618652344 \n",
            "\n",
            "Start of epoch 55 / 1000\n",
            "Train loss 89.82686614990234, time 0.3902733325958252 \n",
            "\n",
            "Start of epoch 56 / 1000\n",
            "Train loss 93.8321762084961, time 0.4029841423034668 \n",
            "\n",
            "Start of epoch 57 / 1000\n",
            "Train loss 89.50691223144531, time 0.4408905506134033 \n",
            "\n",
            "Start of epoch 58 / 1000\n",
            "Train loss 91.57842254638672, time 0.39718079566955566 \n",
            "\n",
            "Start of epoch 59 / 1000\n",
            "Train loss 86.77314758300781, time 0.41005444526672363 \n",
            "\n",
            "Start of epoch 60 / 1000\n",
            "Train loss 88.49168395996094, time 0.5346052646636963 \n",
            "\n",
            "Start of epoch 61 / 1000\n",
            "Train loss 106.04862213134766, time 0.5226285457611084 \n",
            "\n",
            "Start of epoch 62 / 1000\n",
            "Train loss 87.03529357910156, time 0.5487532615661621 \n",
            "\n",
            "Start of epoch 63 / 1000\n",
            "Train loss 84.0146713256836, time 0.58762526512146 \n",
            "\n",
            "Start of epoch 64 / 1000\n",
            "Train loss 81.9818344116211, time 0.5702130794525146 \n",
            "\n",
            "Start of epoch 65 / 1000\n",
            "Train loss 85.02928161621094, time 0.3994307518005371 \n",
            "\n",
            "Start of epoch 66 / 1000\n",
            "Train loss 83.56098937988281, time 0.3929011821746826 \n",
            "\n",
            "Start of epoch 67 / 1000\n",
            "Train loss 81.96968078613281, time 0.4005599021911621 \n",
            "\n",
            "Start of epoch 68 / 1000\n",
            "Train loss 79.23960876464844, time 0.4169015884399414 \n",
            "\n",
            "Start of epoch 69 / 1000\n",
            "Train loss 81.02629852294922, time 0.4088258743286133 \n",
            "\n",
            "Start of epoch 70 / 1000\n",
            "Train loss 83.29549407958984, time 0.402799129486084 \n",
            "\n",
            "Start of epoch 71 / 1000\n",
            "Train loss 77.68145751953125, time 0.39876270294189453 \n",
            "\n",
            "Start of epoch 72 / 1000\n",
            "Train loss 78.63935852050781, time 0.4031076431274414 \n",
            "\n",
            "Start of epoch 73 / 1000\n",
            "Train loss 75.79693603515625, time 0.40572333335876465 \n",
            "\n",
            "Start of epoch 74 / 1000\n",
            "Train loss 74.79660034179688, time 0.4115562438964844 \n",
            "\n",
            "Start of epoch 75 / 1000\n",
            "Train loss 73.8990478515625, time 0.4028444290161133 \n",
            "\n",
            "Number of samples in test set: 0\n",
            "mean loss: nan\n",
            "mean CER: nan\n",
            "WER: nan\n",
            "\n",
            "Validation loss is greater than best_val_loss\n",
            "Start of epoch 76 / 1000\n",
            "Train loss 76.25980377197266, time 0.4084169864654541 \n",
            "\n",
            "Start of epoch 77 / 1000\n",
            "Train loss 77.10491943359375, time 0.3992469310760498 \n",
            "\n",
            "Start of epoch 78 / 1000\n",
            "Train loss 71.57221984863281, time 0.4457991123199463 \n",
            "\n",
            "Start of epoch 79 / 1000\n",
            "Train loss 74.45523071289062, time 0.3990652561187744 \n",
            "\n",
            "Start of epoch 80 / 1000\n",
            "Train loss 69.81238555908203, time 0.39858222007751465 \n",
            "\n",
            "Start of epoch 81 / 1000\n",
            "Train loss 74.22312927246094, time 0.4209897518157959 \n",
            "\n",
            "Start of epoch 82 / 1000\n",
            "Train loss 78.03514099121094, time 0.388507604598999 \n",
            "\n",
            "Start of epoch 83 / 1000\n",
            "Train loss 67.5315933227539, time 0.40347814559936523 \n",
            "\n",
            "Start of epoch 84 / 1000\n",
            "Train loss 67.15521240234375, time 0.3982861042022705 \n",
            "\n",
            "Start of epoch 85 / 1000\n",
            "Train loss 78.90943145751953, time 0.3835568428039551 \n",
            "\n",
            "Start of epoch 86 / 1000\n",
            "Train loss 67.231201171875, time 0.4034905433654785 \n",
            "\n",
            "Start of epoch 87 / 1000\n",
            "Train loss 67.628173828125, time 0.4038560390472412 \n",
            "\n",
            "Start of epoch 88 / 1000\n",
            "Train loss 75.02413177490234, time 0.4055790901184082 \n",
            "\n",
            "Start of epoch 89 / 1000\n",
            "Train loss 64.53871154785156, time 0.5130927562713623 \n",
            "\n",
            "Start of epoch 90 / 1000\n",
            "Train loss 63.87085723876953, time 0.5407490730285645 \n",
            "\n",
            "Start of epoch 91 / 1000\n",
            "Train loss 88.93128967285156, time 0.5497996807098389 \n",
            "\n",
            "Start of epoch 92 / 1000\n",
            "Train loss 66.86631774902344, time 0.5910022258758545 \n",
            "\n",
            "Start of epoch 93 / 1000\n",
            "Train loss 66.52281951904297, time 0.5956730842590332 \n",
            "\n",
            "Start of epoch 94 / 1000\n",
            "Train loss 65.40763092041016, time 0.4435739517211914 \n",
            "\n",
            "Start of epoch 95 / 1000\n",
            "Train loss 62.53148651123047, time 0.4183170795440674 \n",
            "\n",
            "Start of epoch 96 / 1000\n",
            "Train loss 66.20399475097656, time 0.39972901344299316 \n",
            "\n",
            "Start of epoch 97 / 1000\n",
            "Train loss 69.74583435058594, time 0.4058823585510254 \n",
            "\n",
            "Start of epoch 98 / 1000\n",
            "Train loss 62.49985122680664, time 0.4104325771331787 \n",
            "\n",
            "Start of epoch 99 / 1000\n",
            "Train loss 61.90015411376953, time 0.4152092933654785 \n",
            "\n",
            "Start of epoch 100 / 1000\n",
            "Train loss 68.9025650024414, time 0.397388219833374 \n",
            "\n",
            "Number of samples in test set: 0\n",
            "mean loss: nan\n",
            "mean CER: nan\n",
            "WER: nan\n",
            "\n",
            "Validation loss is greater than best_val_loss\n",
            "Start of epoch 101 / 1000\n",
            "Train loss 65.0066909790039, time 0.39557814598083496 \n",
            "\n",
            "Start of epoch 102 / 1000\n",
            "Train loss 60.598751068115234, time 0.40890073776245117 \n",
            "\n",
            "Start of epoch 103 / 1000\n",
            "Train loss 67.20848083496094, time 0.3891153335571289 \n",
            "\n",
            "Start of epoch 104 / 1000\n",
            "Train loss 60.07157897949219, time 0.4060969352722168 \n",
            "\n",
            "Start of epoch 105 / 1000\n",
            "Train loss 60.43429946899414, time 0.39521002769470215 \n",
            "\n",
            "Start of epoch 106 / 1000\n",
            "Train loss 80.02520751953125, time 0.4020380973815918 \n",
            "\n",
            "Start of epoch 107 / 1000\n",
            "Train loss 59.480186462402344, time 0.408430814743042 \n",
            "\n",
            "Start of epoch 108 / 1000\n",
            "Train loss 59.16546630859375, time 0.4129922389984131 \n",
            "\n",
            "Start of epoch 109 / 1000\n",
            "Train loss 57.75422286987305, time 0.40457916259765625 \n",
            "\n",
            "Start of epoch 110 / 1000\n",
            "Train loss 60.64333724975586, time 0.38677263259887695 \n",
            "\n",
            "Start of epoch 111 / 1000\n",
            "Train loss 61.026161193847656, time 0.37998461723327637 \n",
            "\n",
            "Start of epoch 112 / 1000\n",
            "Train loss 57.85623550415039, time 0.40738964080810547 \n",
            "\n",
            "Start of epoch 113 / 1000\n",
            "Train loss 59.621238708496094, time 0.4055767059326172 \n",
            "\n",
            "Start of epoch 114 / 1000\n",
            "Train loss 56.76638412475586, time 0.4192235469818115 \n",
            "\n",
            "Start of epoch 115 / 1000\n",
            "Train loss 64.86124420166016, time 0.3827543258666992 \n",
            "\n",
            "Start of epoch 116 / 1000\n",
            "Train loss 54.890098571777344, time 0.3926701545715332 \n",
            "\n",
            "Start of epoch 117 / 1000\n",
            "Train loss 58.489723205566406, time 0.4113001823425293 \n",
            "\n",
            "Start of epoch 118 / 1000\n",
            "Train loss 64.70394897460938, time 0.3984951972961426 \n",
            "\n",
            "Start of epoch 119 / 1000\n",
            "Train loss 58.53057098388672, time 0.5483324527740479 \n",
            "\n",
            "Start of epoch 120 / 1000\n",
            "Train loss 54.97850799560547, time 0.5029833316802979 \n",
            "\n",
            "Start of epoch 121 / 1000\n",
            "Train loss 57.52239990234375, time 0.5669054985046387 \n",
            "\n",
            "Start of epoch 122 / 1000\n",
            "Train loss 54.295108795166016, time 0.559274435043335 \n",
            "\n",
            "Start of epoch 123 / 1000\n",
            "Train loss 53.16004180908203, time 0.535851240158081 \n",
            "\n",
            "Start of epoch 124 / 1000\n",
            "Train loss 74.37518310546875, time 0.39612889289855957 \n",
            "\n",
            "Start of epoch 125 / 1000\n",
            "Train loss 55.37730407714844, time 0.40679430961608887 \n",
            "\n",
            "Number of samples in test set: 0\n",
            "mean loss: nan\n",
            "mean CER: nan\n",
            "WER: nan\n",
            "\n",
            "Validation loss is greater than best_val_loss\n",
            "Start of epoch 126 / 1000\n",
            "Train loss 53.44658279418945, time 0.3971550464630127 \n",
            "\n",
            "Start of epoch 127 / 1000\n",
            "Train loss 53.26342010498047, time 0.39255642890930176 \n",
            "\n",
            "Start of epoch 128 / 1000\n",
            "Train loss 55.88483428955078, time 0.41626930236816406 \n",
            "\n",
            "Start of epoch 129 / 1000\n",
            "Train loss 51.89502716064453, time 0.38663578033447266 \n",
            "\n",
            "Start of epoch 130 / 1000\n",
            "Train loss 56.74571228027344, time 0.3938124179840088 \n",
            "\n",
            "Start of epoch 131 / 1000\n",
            "Train loss 54.94916915893555, time 0.38844847679138184 \n",
            "\n",
            "Start of epoch 132 / 1000\n",
            "Train loss 53.27949905395508, time 0.38527989387512207 \n",
            "\n",
            "Start of epoch 133 / 1000\n",
            "Train loss 75.00804138183594, time 0.39957499504089355 \n",
            "\n",
            "Start of epoch 134 / 1000\n",
            "Train loss 53.01576232910156, time 0.3915221691131592 \n",
            "\n",
            "Start of epoch 135 / 1000\n",
            "Train loss 51.42059326171875, time 0.40126776695251465 \n",
            "\n",
            "Start of epoch 136 / 1000\n",
            "Train loss 51.454246520996094, time 0.4105796813964844 \n",
            "\n",
            "Start of epoch 137 / 1000\n",
            "Train loss 54.05009460449219, time 0.39709925651550293 \n",
            "\n",
            "Start of epoch 138 / 1000\n",
            "Train loss 53.05808639526367, time 0.418933629989624 \n",
            "\n",
            "Start of epoch 139 / 1000\n",
            "Train loss 61.110557556152344, time 0.4117300510406494 \n",
            "\n",
            "Start of epoch 140 / 1000\n",
            "Train loss 49.098731994628906, time 0.39987683296203613 \n",
            "\n",
            "Start of epoch 141 / 1000\n",
            "Train loss 49.32544708251953, time 0.39843249320983887 \n",
            "\n",
            "Start of epoch 142 / 1000\n",
            "Train loss 52.37624740600586, time 0.3934507369995117 \n",
            "\n",
            "Start of epoch 143 / 1000\n",
            "Train loss 53.54265213012695, time 0.41783618927001953 \n",
            "\n",
            "Start of epoch 144 / 1000\n",
            "Train loss 49.03147888183594, time 0.3921535015106201 \n",
            "\n",
            "Start of epoch 145 / 1000\n",
            "Train loss 57.935890197753906, time 0.3970448970794678 \n",
            "\n",
            "Start of epoch 146 / 1000\n",
            "Train loss 48.77902603149414, time 0.40185117721557617 \n",
            "\n",
            "Start of epoch 147 / 1000\n",
            "Train loss 47.111698150634766, time 0.3984189033508301 \n",
            "\n",
            "Start of epoch 148 / 1000\n",
            "Train loss 70.45355987548828, time 0.47522735595703125 \n",
            "\n",
            "Start of epoch 149 / 1000\n",
            "Train loss 48.043113708496094, time 0.5352878570556641 \n",
            "\n",
            "Start of epoch 150 / 1000\n",
            "Train loss 48.13010787963867, time 0.5241012573242188 \n",
            "\n",
            "Number of samples in test set: 0\n",
            "mean loss: nan\n",
            "mean CER: nan\n",
            "WER: nan\n",
            "\n",
            "Validation loss is greater than best_val_loss\n",
            "Start of epoch 151 / 1000\n",
            "Train loss 57.87806701660156, time 0.5998668670654297 \n",
            "\n",
            "Start of epoch 152 / 1000\n",
            "Train loss 50.7384147644043, time 0.6036896705627441 \n",
            "\n",
            "Start of epoch 153 / 1000\n",
            "Train loss 47.704734802246094, time 0.5038230419158936 \n",
            "\n",
            "Start of epoch 154 / 1000\n",
            "Train loss 55.027278900146484, time 0.4050130844116211 \n",
            "\n",
            "Start of epoch 155 / 1000\n",
            "Train loss 48.10300827026367, time 0.39780545234680176 \n",
            "\n",
            "Start of epoch 156 / 1000\n",
            "Train loss 47.642051696777344, time 0.391587495803833 \n",
            "\n",
            "Start of epoch 157 / 1000\n",
            "Train loss 50.474853515625, time 0.40823888778686523 \n",
            "\n",
            "Start of epoch 158 / 1000\n",
            "Train loss 47.03886795043945, time 0.39865541458129883 \n",
            "\n",
            "Start of epoch 159 / 1000\n",
            "Train loss 46.86339569091797, time 0.4012336730957031 \n",
            "\n",
            "Start of epoch 160 / 1000\n",
            "Train loss 48.80872344970703, time 0.3963205814361572 \n",
            "\n",
            "Start of epoch 161 / 1000\n",
            "Train loss 50.055267333984375, time 0.39371609687805176 \n",
            "\n",
            "Start of epoch 162 / 1000\n",
            "Train loss 45.97906494140625, time 0.4044308662414551 \n",
            "\n",
            "Start of epoch 163 / 1000\n",
            "Train loss 48.08924865722656, time 0.41673731803894043 \n",
            "\n",
            "Start of epoch 164 / 1000\n",
            "Train loss 45.701053619384766, time 0.40441274642944336 \n",
            "\n",
            "Start of epoch 165 / 1000\n",
            "Train loss 45.54224395751953, time 0.4081897735595703 \n",
            "\n",
            "Start of epoch 166 / 1000\n",
            "Train loss 48.9544792175293, time 0.3920776844024658 \n",
            "\n",
            "Start of epoch 167 / 1000\n",
            "Train loss 45.053306579589844, time 0.3959343433380127 \n",
            "\n",
            "Start of epoch 168 / 1000\n",
            "Train loss 47.55149841308594, time 0.398571252822876 \n",
            "\n",
            "Start of epoch 169 / 1000\n",
            "Train loss 47.062461853027344, time 0.39716553688049316 \n",
            "\n",
            "Start of epoch 170 / 1000\n",
            "Train loss 47.0517578125, time 0.403156042098999 \n",
            "\n",
            "Start of epoch 171 / 1000\n",
            "Train loss 47.869422912597656, time 0.3871607780456543 \n",
            "\n",
            "Start of epoch 172 / 1000\n",
            "Train loss 43.20958709716797, time 0.41056013107299805 \n",
            "\n",
            "Start of epoch 173 / 1000\n",
            "Train loss 44.49553680419922, time 0.40447497367858887 \n",
            "\n",
            "Start of epoch 174 / 1000\n",
            "Train loss 43.26139831542969, time 0.389570951461792 \n",
            "\n",
            "Start of epoch 175 / 1000\n",
            "Train loss 54.276527404785156, time 0.40972232818603516 \n",
            "\n",
            "Number of samples in test set: 0\n",
            "mean loss: nan\n",
            "mean CER: nan\n",
            "WER: nan\n",
            "\n",
            "Validation loss is greater than best_val_loss\n",
            "Start of epoch 176 / 1000\n",
            "Train loss 43.686988830566406, time 0.42098474502563477 \n",
            "\n",
            "Start of epoch 177 / 1000\n",
            "Train loss 46.48463439941406, time 0.42143774032592773 \n",
            "\n",
            "Start of epoch 178 / 1000\n",
            "Train loss 42.460853576660156, time 0.5189874172210693 \n",
            "\n",
            "Start of epoch 179 / 1000\n",
            "Train loss 42.940040588378906, time 0.5263862609863281 \n",
            "\n",
            "Start of epoch 180 / 1000\n",
            "Train loss 43.11912536621094, time 0.5441322326660156 \n",
            "\n",
            "Start of epoch 181 / 1000\n",
            "Train loss 48.469520568847656, time 0.5984475612640381 \n",
            "\n",
            "Start of epoch 182 / 1000\n",
            "Train loss 44.78150177001953, time 0.5402767658233643 \n",
            "\n",
            "Start of epoch 183 / 1000\n",
            "Train loss 43.597877502441406, time 0.4017801284790039 \n",
            "\n",
            "Start of epoch 184 / 1000\n",
            "Train loss 44.36708068847656, time 0.3908660411834717 \n",
            "\n",
            "Start of epoch 185 / 1000\n",
            "Train loss 45.044158935546875, time 0.3994162082672119 \n",
            "\n",
            "Start of epoch 186 / 1000\n",
            "Train loss 42.34477996826172, time 0.39687657356262207 \n",
            "\n",
            "Start of epoch 187 / 1000\n",
            "Train loss 43.32049560546875, time 0.3930933475494385 \n",
            "\n",
            "Start of epoch 188 / 1000\n",
            "Train loss 42.220680236816406, time 0.4243152141571045 \n",
            "\n",
            "Start of epoch 189 / 1000\n",
            "Train loss 42.091529846191406, time 0.4002549648284912 \n",
            "\n",
            "Start of epoch 190 / 1000\n",
            "Train loss 43.41341018676758, time 0.3845794200897217 \n",
            "\n",
            "Start of epoch 191 / 1000\n",
            "Train loss 41.16815948486328, time 0.4064910411834717 \n",
            "\n",
            "Start of epoch 192 / 1000\n",
            "Train loss 40.66783905029297, time 0.3899526596069336 \n",
            "\n",
            "Start of epoch 193 / 1000\n",
            "Train loss 43.90244674682617, time 0.3937489986419678 \n",
            "\n",
            "Start of epoch 194 / 1000\n",
            "Train loss 40.992286682128906, time 0.4066464900970459 \n",
            "\n",
            "Start of epoch 195 / 1000\n",
            "Train loss 39.98320007324219, time 0.3921377658843994 \n",
            "\n",
            "Start of epoch 196 / 1000\n",
            "Train loss 65.83139038085938, time 0.411257266998291 \n",
            "\n",
            "Start of epoch 197 / 1000\n",
            "Train loss 39.77790069580078, time 0.39493298530578613 \n",
            "\n",
            "Start of epoch 198 / 1000\n",
            "Train loss 39.95713424682617, time 0.39224720001220703 \n",
            "\n",
            "Start of epoch 199 / 1000\n",
            "Train loss 59.0542106628418, time 0.4049985408782959 \n",
            "\n",
            "Start of epoch 200 / 1000\n",
            "Train loss 39.65669250488281, time 0.40811848640441895 \n",
            "\n",
            "Number of samples in test set: 0\n",
            "mean loss: nan\n",
            "mean CER: nan\n",
            "WER: nan\n",
            "\n",
            "Validation loss is greater than best_val_loss\n",
            "Start of epoch 201 / 1000\n",
            "Train loss 40.471134185791016, time 0.39875316619873047 \n",
            "\n",
            "Start of epoch 202 / 1000\n",
            "Train loss 39.06298065185547, time 0.38256216049194336 \n",
            "\n",
            "Start of epoch 203 / 1000\n",
            "Train loss 42.57925796508789, time 0.3806929588317871 \n",
            "\n",
            "Start of epoch 204 / 1000\n",
            "Train loss 38.35265350341797, time 0.40682196617126465 \n",
            "\n",
            "Start of epoch 205 / 1000\n",
            "Train loss 41.91932678222656, time 0.3927919864654541 \n",
            "\n",
            "Start of epoch 206 / 1000\n",
            "Train loss 39.584228515625, time 0.4029068946838379 \n",
            "\n",
            "Start of epoch 207 / 1000\n",
            "Train loss 39.767581939697266, time 0.4548459053039551 \n",
            "\n",
            "Start of epoch 208 / 1000\n",
            "Train loss 41.77476501464844, time 0.5499496459960938 \n",
            "\n",
            "Start of epoch 209 / 1000\n",
            "Train loss 38.01426696777344, time 0.5227768421173096 \n",
            "\n",
            "Start of epoch 210 / 1000\n",
            "Train loss 41.105125427246094, time 0.594437837600708 \n",
            "\n",
            "Start of epoch 211 / 1000\n",
            "Train loss 39.6473503112793, time 0.5747199058532715 \n",
            "\n",
            "Start of epoch 212 / 1000\n",
            "Train loss 40.68666076660156, time 0.5151631832122803 \n",
            "\n",
            "Start of epoch 213 / 1000\n",
            "Train loss 38.39280700683594, time 0.3970041275024414 \n",
            "\n",
            "Start of epoch 214 / 1000\n",
            "Train loss 41.30242156982422, time 0.39551329612731934 \n",
            "\n",
            "Start of epoch 215 / 1000\n",
            "Train loss 37.31214904785156, time 0.40138936042785645 \n",
            "\n",
            "Start of epoch 216 / 1000\n",
            "Train loss 40.52210235595703, time 0.3978118896484375 \n",
            "\n",
            "Start of epoch 217 / 1000\n",
            "Train loss 37.44408416748047, time 0.4173269271850586 \n",
            "\n",
            "Start of epoch 218 / 1000\n",
            "Train loss 37.30555725097656, time 0.4080023765563965 \n",
            "\n",
            "Start of epoch 219 / 1000\n",
            "Train loss 38.71090316772461, time 0.4275188446044922 \n",
            "\n",
            "Start of epoch 220 / 1000\n",
            "Train loss 42.42706298828125, time 0.3949713706970215 \n",
            "\n",
            "Start of epoch 221 / 1000\n",
            "Train loss 40.176666259765625, time 0.3893003463745117 \n",
            "\n",
            "Start of epoch 222 / 1000\n",
            "Train loss 37.86075210571289, time 0.3986377716064453 \n",
            "\n",
            "Start of epoch 223 / 1000\n",
            "Train loss 36.47613525390625, time 0.3851349353790283 \n",
            "\n",
            "Start of epoch 224 / 1000\n",
            "Train loss 39.194664001464844, time 0.3993234634399414 \n",
            "\n",
            "Start of epoch 225 / 1000\n",
            "Train loss 38.93569564819336, time 0.4024331569671631 \n",
            "\n",
            "Number of samples in test set: 0\n",
            "mean loss: nan\n",
            "mean CER: nan\n",
            "WER: nan\n",
            "\n",
            "Validation loss is greater than best_val_loss\n",
            "Start of epoch 226 / 1000\n",
            "Train loss 37.065608978271484, time 0.38733601570129395 \n",
            "\n",
            "Start of epoch 227 / 1000\n",
            "Train loss 38.61697769165039, time 0.39498066902160645 \n",
            "\n",
            "Start of epoch 228 / 1000\n",
            "Train loss 37.322364807128906, time 0.40201759338378906 \n",
            "\n",
            "Start of epoch 229 / 1000\n",
            "Train loss 36.219146728515625, time 0.40677976608276367 \n",
            "\n",
            "Start of epoch 230 / 1000\n",
            "Train loss 38.55535125732422, time 0.4152665138244629 \n",
            "\n",
            "Start of epoch 231 / 1000\n",
            "Train loss 38.46784973144531, time 0.39993786811828613 \n",
            "\n",
            "Start of epoch 232 / 1000\n",
            "Train loss 37.95831298828125, time 0.427304744720459 \n",
            "\n",
            "Start of epoch 233 / 1000\n",
            "Train loss 38.1381721496582, time 0.4490342140197754 \n",
            "\n",
            "Start of epoch 234 / 1000\n",
            "Train loss 34.66896438598633, time 0.4535207748413086 \n",
            "\n",
            "Start of epoch 235 / 1000\n",
            "Train loss 38.188819885253906, time 0.41995692253112793 \n",
            "\n",
            "Start of epoch 236 / 1000\n",
            "Train loss 35.54916000366211, time 0.4425983428955078 \n",
            "\n",
            "Start of epoch 237 / 1000\n",
            "Train loss 35.37018966674805, time 0.5531446933746338 \n",
            "\n",
            "Start of epoch 238 / 1000\n",
            "Train loss 37.92909240722656, time 0.5587594509124756 \n",
            "\n",
            "Start of epoch 239 / 1000\n",
            "Train loss 33.8154411315918, time 0.607266902923584 \n",
            "\n",
            "Start of epoch 240 / 1000\n",
            "Train loss 36.23707962036133, time 0.5904698371887207 \n",
            "\n",
            "Start of epoch 241 / 1000\n",
            "Train loss 54.06605529785156, time 0.550973653793335 \n",
            "\n",
            "Start of epoch 242 / 1000\n",
            "Train loss 35.762046813964844, time 0.3988761901855469 \n",
            "\n",
            "Start of epoch 243 / 1000\n",
            "Train loss 37.10097122192383, time 0.40900278091430664 \n",
            "\n",
            "Start of epoch 244 / 1000\n",
            "Train loss 37.10499572753906, time 0.41980695724487305 \n",
            "\n",
            "Start of epoch 245 / 1000\n",
            "Train loss 34.032203674316406, time 0.42817091941833496 \n",
            "\n",
            "Start of epoch 246 / 1000\n",
            "Train loss 35.767547607421875, time 0.44221067428588867 \n",
            "\n",
            "Start of epoch 247 / 1000\n",
            "Train loss 33.9952507019043, time 0.4077117443084717 \n",
            "\n",
            "Start of epoch 248 / 1000\n",
            "Train loss 34.293704986572266, time 0.4089968204498291 \n",
            "\n",
            "Start of epoch 249 / 1000\n",
            "Train loss 33.95290756225586, time 0.40349507331848145 \n",
            "\n",
            "Start of epoch 250 / 1000\n",
            "Train loss 35.27943420410156, time 0.461749792098999 \n",
            "\n",
            "Number of samples in test set: 0\n",
            "mean loss: nan\n",
            "mean CER: nan\n",
            "WER: nan\n",
            "\n",
            "Validation loss is greater than best_val_loss\n",
            "Start of epoch 251 / 1000\n",
            "Train loss 35.85076141357422, time 0.4064807891845703 \n",
            "\n",
            "Start of epoch 252 / 1000\n",
            "Train loss 35.404502868652344, time 0.430556058883667 \n",
            "\n",
            "Start of epoch 253 / 1000\n",
            "Train loss 32.265357971191406, time 0.465228796005249 \n",
            "\n",
            "Start of epoch 254 / 1000\n",
            "Train loss 33.305381774902344, time 0.4348335266113281 \n",
            "\n",
            "Start of epoch 255 / 1000\n",
            "Train loss 32.22567367553711, time 0.43195271492004395 \n",
            "\n",
            "Start of epoch 256 / 1000\n",
            "Train loss 50.903934478759766, time 0.42525362968444824 \n",
            "\n",
            "Start of epoch 257 / 1000\n",
            "Train loss 32.4510498046875, time 0.4352872371673584 \n",
            "\n",
            "Start of epoch 258 / 1000\n",
            "Train loss 31.570453643798828, time 0.4430692195892334 \n",
            "\n",
            "Start of epoch 259 / 1000\n",
            "Train loss 49.611114501953125, time 0.4290738105773926 \n",
            "\n",
            "Start of epoch 260 / 1000\n",
            "Train loss 31.749961853027344, time 0.45944881439208984 \n",
            "\n",
            "Start of epoch 261 / 1000\n",
            "Train loss 33.66886901855469, time 0.4403114318847656 \n",
            "\n",
            "Start of epoch 262 / 1000\n",
            "Train loss 32.39167022705078, time 0.4402599334716797 \n",
            "\n",
            "Start of epoch 263 / 1000\n",
            "Train loss 32.73273468017578, time 0.43776392936706543 \n",
            "\n",
            "Start of epoch 264 / 1000\n",
            "Train loss 31.640018463134766, time 0.4842674732208252 \n",
            "\n",
            "Start of epoch 265 / 1000\n",
            "Train loss 46.436859130859375, time 0.5907313823699951 \n",
            "\n",
            "Start of epoch 266 / 1000\n",
            "Train loss 31.01706314086914, time 0.5886850357055664 \n",
            "\n",
            "Start of epoch 267 / 1000\n",
            "Train loss 34.72298049926758, time 0.6462254524230957 \n",
            "\n",
            "Start of epoch 268 / 1000\n",
            "Train loss 34.24769973754883, time 0.654104471206665 \n",
            "\n",
            "Start of epoch 269 / 1000\n",
            "Train loss 31.450557708740234, time 0.5924046039581299 \n",
            "\n",
            "Start of epoch 270 / 1000\n",
            "Train loss 31.327491760253906, time 0.46094393730163574 \n",
            "\n",
            "Start of epoch 271 / 1000\n",
            "Train loss 34.10865020751953, time 0.4493293762207031 \n",
            "\n",
            "Start of epoch 272 / 1000\n",
            "Train loss 31.105810165405273, time 0.43701791763305664 \n",
            "\n",
            "Start of epoch 273 / 1000\n",
            "Train loss 32.08856964111328, time 0.4295804500579834 \n",
            "\n",
            "Start of epoch 274 / 1000\n",
            "Train loss 35.357269287109375, time 0.453155517578125 \n",
            "\n",
            "Start of epoch 275 / 1000\n",
            "Train loss 33.74579620361328, time 0.44069385528564453 \n",
            "\n",
            "Number of samples in test set: 0\n",
            "mean loss: nan\n",
            "mean CER: nan\n",
            "WER: nan\n",
            "\n",
            "Validation loss is greater than best_val_loss\n",
            "Start of epoch 276 / 1000\n",
            "Train loss 30.86358642578125, time 0.44800710678100586 \n",
            "\n",
            "Start of epoch 277 / 1000\n",
            "Train loss 32.48233413696289, time 0.41275930404663086 \n",
            "\n",
            "Start of epoch 278 / 1000\n",
            "Train loss 32.90730285644531, time 0.43589091300964355 \n",
            "\n",
            "Start of epoch 279 / 1000\n",
            "Train loss 30.2069091796875, time 0.4422605037689209 \n",
            "\n",
            "Start of epoch 280 / 1000\n",
            "Train loss 33.145484924316406, time 0.43498682975769043 \n",
            "\n",
            "Start of epoch 281 / 1000\n",
            "Train loss 30.599050521850586, time 0.4537079334259033 \n",
            "\n",
            "Start of epoch 282 / 1000\n",
            "Train loss 29.89008331298828, time 0.45017337799072266 \n",
            "\n",
            "Start of epoch 283 / 1000\n",
            "Train loss 32.5034065246582, time 0.4726860523223877 \n",
            "\n",
            "Start of epoch 284 / 1000\n",
            "Train loss 30.93789291381836, time 0.45002317428588867 \n",
            "\n",
            "Start of epoch 285 / 1000\n",
            "Train loss 29.899654388427734, time 0.4726588726043701 \n",
            "\n",
            "Start of epoch 286 / 1000\n",
            "Train loss 30.42534065246582, time 0.45248985290527344 \n",
            "\n",
            "Start of epoch 287 / 1000\n",
            "Train loss 31.02042579650879, time 0.432081937789917 \n",
            "\n",
            "Start of epoch 288 / 1000\n",
            "Train loss 29.689313888549805, time 0.42435216903686523 \n",
            "\n",
            "Start of epoch 289 / 1000\n",
            "Train loss 37.353057861328125, time 0.45585155487060547 \n",
            "\n",
            "Start of epoch 290 / 1000\n",
            "Train loss 28.749483108520508, time 0.42018723487854004 \n",
            "\n",
            "Start of epoch 291 / 1000\n",
            "Train loss 30.86977767944336, time 0.4230988025665283 \n",
            "\n",
            "Start of epoch 292 / 1000\n",
            "Train loss 31.47500991821289, time 0.5755701065063477 \n",
            "\n",
            "Start of epoch 293 / 1000\n",
            "Train loss 29.37948989868164, time 0.5656938552856445 \n",
            "\n",
            "Start of epoch 294 / 1000\n",
            "Train loss 29.318458557128906, time 0.6123905181884766 \n",
            "\n",
            "Start of epoch 295 / 1000\n",
            "Train loss 35.061161041259766, time 0.6205296516418457 \n",
            "\n",
            "Start of epoch 296 / 1000\n",
            "Train loss 28.21537208557129, time 0.5653188228607178 \n",
            "\n",
            "Start of epoch 297 / 1000\n",
            "Train loss 31.20703887939453, time 0.4510016441345215 \n",
            "\n",
            "Start of epoch 298 / 1000\n",
            "Train loss 35.420631408691406, time 0.4249436855316162 \n",
            "\n",
            "Start of epoch 299 / 1000\n",
            "Train loss 28.312007904052734, time 0.41503214836120605 \n",
            "\n",
            "Start of epoch 300 / 1000\n",
            "Train loss 31.079776763916016, time 0.4560122489929199 \n",
            "\n",
            "Number of samples in test set: 0\n",
            "mean loss: nan\n",
            "mean CER: nan\n",
            "WER: nan\n",
            "\n",
            "Validation loss is greater than best_val_loss\n",
            "Start of epoch 301 / 1000\n",
            "Train loss 34.198081970214844, time 0.44849658012390137 \n",
            "\n",
            "Start of epoch 302 / 1000\n",
            "Train loss 28.543071746826172, time 0.4223346710205078 \n",
            "\n",
            "Start of epoch 303 / 1000\n",
            "Train loss 30.10421371459961, time 0.43268728256225586 \n",
            "\n",
            "Start of epoch 304 / 1000\n",
            "Train loss 28.818634033203125, time 0.42590999603271484 \n",
            "\n",
            "Start of epoch 305 / 1000\n",
            "Train loss 30.503915786743164, time 0.4434077739715576 \n",
            "\n",
            "Start of epoch 306 / 1000\n",
            "Train loss 30.52086639404297, time 0.4542374610900879 \n",
            "\n",
            "Start of epoch 307 / 1000\n",
            "Train loss 30.438867568969727, time 0.4191889762878418 \n",
            "\n",
            "Start of epoch 308 / 1000\n",
            "Train loss 27.590206146240234, time 0.4346497058868408 \n",
            "\n",
            "Start of epoch 309 / 1000\n",
            "Train loss 27.552087783813477, time 0.43869876861572266 \n",
            "\n",
            "Start of epoch 310 / 1000\n",
            "Train loss 29.432899475097656, time 0.43834877014160156 \n",
            "\n",
            "Start of epoch 311 / 1000\n",
            "Train loss 27.761795043945312, time 0.40894126892089844 \n",
            "\n",
            "Start of epoch 312 / 1000\n",
            "Train loss 29.127750396728516, time 0.4190335273742676 \n",
            "\n",
            "Start of epoch 313 / 1000\n",
            "Train loss 28.100341796875, time 0.4332005977630615 \n",
            "\n",
            "Start of epoch 314 / 1000\n",
            "Train loss 27.26828384399414, time 0.40634632110595703 \n",
            "\n",
            "Start of epoch 315 / 1000\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py\", line 351, in apply\n",
            "    self._backend_apply_gradients(grads, trainable_variables)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py\", line 405, in _backend_apply_gradients\n",
            "    self._backend_update_step(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/optimizer.py\", line 119, in _backend_update_step\n",
            "    tf.__internal__.distribute.interim.maybe_merge_call(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/merge_call_interim.py\", line 51, in maybe_merge_call\n",
            "    return fn(strategy, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/optimizer.py\", line 135, in _distributed_tf_update_step\n",
            "    distribution.extended.update(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/distribute_lib.py\", line 3005, in update\n",
            "    return self._update(var, fn, args, kwargs, group)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/distribute_lib.py\", line 4075, in _update\n",
            "    return self._update_non_slot(var, fn, (var,) + tuple(args), kwargs, group)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/distribute_lib.py\", line 4081, in _update_non_slot\n",
            "    result = fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\", line 596, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/optimizer.py\", line 132, in apply_grad_to_update_var\n",
            "    return self.update_step(grad, var, learning_rate)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/adam.py\", line 130, in update_step\n",
            "    alpha = lr * ops.sqrt(1 - beta_2_power) / (1 - beta_1_power)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/override_binary_operator.py\", line 113, in binary_op_wrapper\n",
            "    return func(x, y, name=name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/tensor_math_operator_overrides.py\", line 64, in _mul_dispatch_factory\n",
            "    return math_ops._mul_dispatch(x, y, name=name)  # pylint: disable=protected-access\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/math_ops.py\", line 1711, in _mul_dispatch\n",
            "    return multiply(x, y, name=name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/weak_tensor_ops.py\", line 142, in wrapper\n",
            "    return op(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py\", line 1254, in op_dispatch_handler\n",
            "    result = api_dispatcher.Dispatch(args, kwargs)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/LPRNet_ICT/train.py\", line 156, in <module>\n",
            "    train()\n",
            "  File \"/content/LPRNet_ICT/train.py\", line 111, in train\n",
            "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py\", line 282, in apply_gradients\n",
            "    self.apply(grads, trainable_variables)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py\", line 325, in apply\n",
            "    with backend.name_scope(self.name, caller=self):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/core.py\", line 450, in __exit__\n",
            "    self._tf_name_scope.__exit__(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\", line 5785, in __exit__\n",
            "    self._exit_fns.pop()(type_arg, value_arg, traceback_arg)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\", line 5773, in _restore_name_scope\n",
            "    ctx.scope_name = old_name\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\", line 1094, in scope_name\n",
            "    @scope_name.setter\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/LPRNet_ICT/predict.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UoKFXNGD528S",
        "outputId": "008de2f8-b576-4b22-aea2-a44932a5c40a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-08-02 06:26:33.714490: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-08-02 06:26:33.735954: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-08-02 06:26:33.742526: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-08-02 06:26:33.757096: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-08-02 06:26:34.816804: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Starting program...\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1722579996.258429    2900 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "I0000 00:00:1722579996.305316    2900 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "I0000 00:00:1722579996.305592    2900 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "I0000 00:00:1722579996.306392    2900 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "I0000 00:00:1722579996.306597    2900 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "I0000 00:00:1722579996.306736    2900 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "I0000 00:00:1722579996.500796    2900 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "I0000 00:00:1722579996.501078    2900 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-08-02 06:26:36.501226: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "I0000 00:00:1722579996.501350    2900 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-08-02 06:26:36.501502: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13949 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "Loaded Weights successfully\n",
            "Actual Label \t Predicted Label \n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1722579997.573168    2931 service.cc:146] XLA service 0x7de968002680 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "I0000 00:00:1722579997.573245    2931 service.cc:154]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2024-08-02 06:26:37.611755: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "2024-08-02 06:26:37.795836: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8906\n",
            "I0000 00:00:1722579998.837362    2931 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "KA01ML6902 \t KA01ML6902\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "KA01MP2936 \t KA01M2936\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "KA01ML5359 \t KA01ML5359\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "KA01MP2936 \t KA01M2936\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "KA01MR2693 \t KA01MR2693\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "KA53MD7540 \t KA53MD7540\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "KA53Z0023 \t KKA0L9K2K129\n",
            "total time taken : 2.219449281692505\n"
          ]
        }
      ]
    }
  ]
}